{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe977190",
   "metadata": {},
   "source": [
    "# DATA 5600: Introduction to Regression and Machine Learning for Analytics\n",
    "\n",
    "## __Bayesian Econometrics - Chapter 2 (Koop): The Normal Linear Regression Model__\n",
    "\n",
    "<br>\n",
    "\n",
    "Author:  Tyler J. Brough <br>\n",
    "Updated: November 29, 2021 <br>\n",
    "\n",
    "---\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9651523",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "\n",
    "plt.rcParams['figure.figsize'] = [10, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb7662d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6ad087",
   "metadata": {},
   "source": [
    "## __Introduction__\n",
    "\n",
    "<br>\n",
    "\n",
    "These notes are based upon Chapter 2: The Normal Linear Regression Model with Natural Conjugate Prior and a Single Explanatory Variable from the book _Bayesian Econometrics_ by Gary Koop.\n",
    "\n",
    "<br>\n",
    "\n",
    "* In this chapter and notebook we will look at how to carry out a simple linear regression from the Bayesian perspective\n",
    "\n",
    "* The model consider here has a natural conjugate prior so there is an analytical solution\n",
    "\n",
    "* This will pave the way to move on to the case of multiple linear regression from the Bayesian perspective (the subject of Chapter 3)\n",
    "\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ef0518",
   "metadata": {},
   "source": [
    "## __Section 2.2: The Likelihood Function__\n",
    "\n",
    "<br>\n",
    "\n",
    "To simplify the analysis we will not consider an intercept, so the model we will work with becomes the following: \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\Large{y_{i} = \\beta x_{i} + \\varepsilon_{i}}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "* where $\\varepsilon_{i}$ is an error term\n",
    "\n",
    "* The error term might reflect measurement error or that the true relationship between $x$ and $y$ is only an approximate one\n",
    "\n",
    "* You can also think graphically in terms of the $XY$-plot where we seek to draw a best fitting line by minimizing the squared errors\n",
    "\n",
    "* Assumptions about $\\varepsilon_{i}$ and $x_{i}$ determine the form of the likelihood function\n",
    "\n",
    "\n",
    "1. $\\varepsilon_{i} \\sim N(0,\\sigma^{2})$ (Normally distributed) and further that $\\varepsilon_{i}$ and $\\varepsilon_{j}$ are independent for $i \\ne j$ (iid)\n",
    "\n",
    "\n",
    "2. The $x_{i}$ are either fixed (i.e. not random variables) or, if they are random variables, they are independent of $\\varepsilon_{i}$ with a probability density function, $p(x | \\lambda)$ where $\\lambda$ is a vector of parameters that does not include $\\beta$ and $\\sigma^{2}$\n",
    "\n",
    "<br>\n",
    "\n",
    "The likelihood function then becomes: $\\large{p(y, x | \\beta, \\sigma^{2}, \\lambda)}$\n",
    "\n",
    "<br>\n",
    "\n",
    "The second assumption implies that we can write the likelihood function as:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\Large{p(y, x | \\beta, \\sigma^{2}, \\lambda) = p(y | x, \\beta, \\sigma^{2}) p(x | \\lambda)}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "* We are not directly interested in modeling $x$ so we can work with the likelihood function conditional on $x$\n",
    "\n",
    "\n",
    "* For simplicity then we will not usually even write $x$ in the model\n",
    "\n",
    "* __NB:__ regression modeling (Bayesian or frequentist) works with the conditional distribution of $y$ given $x$ and not their joint distribution\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce2f30a0",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "We can use the above to work out a form for the likelihood function:\n",
    "\n",
    "<br>\n",
    "\n",
    "1. $p(y_{i} | \\beta, \\sigma^{2})$ is Normal \n",
    "\n",
    "2. $E(y_{i} | \\beta, \\sigma^{2}) = \\beta x_{i}$\n",
    "\n",
    "3. $var(y_{i} | \\beta, \\sigma^{2}) = \\sigma^{2}$\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\Large{p(y_{i} | \\beta, \\sigma^{2}) = \\frac{1}{\\sqrt{2 \\pi \\sigma^{2}}} exp\\left[-\\frac{y_{i} - \\beta x_{i})^{2}}{2 \\sigma^{2}} \\right]}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "* Since for $i = j$, $\\epsilon_{i}$ and $\\epsilon_{j}$ are independent of one another it follows that $y_{i}$ and $y_{j}$ are also independent thus\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\Large{p(y | \\beta, \\sigma^{2}) = \\prod_{i=1}^{N} p(y_{i} | \\beta, \\sigma^{2})}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Hence, the likelihood function can be summarized as follows: \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\Large{p(y | \\beta, \\sigma^{2}) = \\frac{1}{(2 \\pi)^{\\frac{N}{2}} \\sigma^{N}}} exp\\left[-\\frac{1}{2\\sigma^{2}} \\sum_{i=1}^{N} (y_{i} - \\beta x_{i})^{2} \\right]\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "For convenience, we will want to slightly rewrite the likelihood function as follows: \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\Large{\\sum_{i=1}^{N} (y_{i} - \\beta x_{i})^{2} = \\nu s^{2} + (\\beta - \\hat{\\beta})^{2} \\sum_{i=1}^{N} x_{i}^{2}}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "where\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\Large{\\begin{align}\n",
    "\\nu &= N - 1 \\\\\n",
    "    & \\\\\n",
    "\\hat{\\beta} &= \\frac{\\sum x_{i} y_{i}}{\\sum x_{i}^{2}} \\\\\n",
    "    & \\\\\n",
    "s^{2} &= \\frac{\\sum (y_{i} - \\hat{\\beta} x_{i})^{2}}{\\nu}\n",
    "\\end{align}}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "* __NB:__ $\\hat{\\beta}$, $s^{2}$ and $\\nu$ are the frequentist OLS estimator for $\\beta$, standard error, and degrees of freedom respectively\n",
    "\n",
    "* Here they are given the interpretation of sufficient statistics\n",
    "\n",
    "* In many cases it is easier (algebraically) to work with the precision rather than the variance\n",
    "\n",
    "* The precision of the error term is defined as $\\large{h = \\frac{1}{\\sigma^{2}}}$\n",
    "\n",
    "* This just helps to ease the mathematical manipulations by reparameterizing things\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78512fe3",
   "metadata": {},
   "source": [
    "### __Sufficient Statistic__\n",
    "\n",
    "---\n",
    "\n",
    "A statistic that, in a certain sense, summarizes all the information contained in a sample of observations about a particular parameter. In more formal terms this can be expressed using the conditional distribution ofo the sample given the statistic and the parameter $f(y | s, \\theta)$,\n",
    "in the sense that $s$ is sufficient for $\\theta$ if this conditional distribution does not depend \n",
    "on $\\theta$. As an example consider a random variable $x$ having the following gamma distribution;\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "f(x) = x^{\\gamma - 1} exp(-x)/\\Gamma(\\gamma)\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "The likelihood for $\\gamma$ given a sample $x_{1}, x_{2}, \\ldots, x_{n}$ is given by\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "exp\\left(-\\sum_{i=1}^{n}x_{i} \\right)\\left[\\prod_{i=1}^{n}x_{i}\\right]^{\\gamma - 1} / \\left[\\Gamma(\\gamma)\\right]^{n}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Thus in this case the geometric mean of the observations is sufficient for the parameter. Such a statistic, which is a function of all other such statistics, is referred to as a _minimal sufficient statistic_.\n",
    "\n",
    "---\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baf77d6",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Here is the Wikipedia article on [Sufficient Statistics](https://en.wikipedia.org/wiki/Sufficient_statistic)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ce87207",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Using these results we can write the likelihood function as: \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\Large{p(y | \\beta, h) \\frac{1}{(2\\pi)^\\frac{N}{2}}} \\{h^{\\frac{1}{2}} exp \\left[-\\frac{h}{2} (\\beta - \\hat{\\beta})^{2}\\sum_{i=1}^{N} x_{i}^{2}\\right] \\} \\{h^{\\frac{\\nu}{2}} exp \\left[-\\frac{h\\nu}{2 s^{-2}} \\right] \\}\n",
    "$$\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2abe7cd",
   "metadata": {},
   "source": [
    "## __Section 2.3: The Prior__\n",
    "\n",
    "<br>\n",
    "\n",
    "* Priors are meant to reflect any information the researcher/analyst has before seeing the data \n",
    "\n",
    "* Priors can take any form \n",
    "\n",
    "* It is common though to choose a functional form for the prior that eases computation and interpretation\n",
    "\n",
    "* _Natural conjugate priors_ have these properties\n",
    "\n",
    "* A conjugate prior is one what when combined with the likelihood yields a posterior that falls in the same class of distributions \n",
    "\n",
    "* A _natural conjugate prior_ has the additional property that it also has the same form as the likelihood function\n",
    "\n",
    "* This means that the prior can be interpreted in the same way as the likelihood\n",
    "\n",
    "* In other words, the prior can be thought of as arising from a fictitious data set from the same process that generated the sample data\n",
    "\n",
    "<br>\n",
    "\n",
    "For the simple linear regression model we must elicit a prior for $\\beta$ and $h$, which we denote by $p(\\beta, h)$\n",
    "\n",
    "* __NB:__ we are not conditioning on the data \n",
    "\n",
    "* The posterior will condition on the data: $p(\\beta, h, | y)$\n",
    "\n",
    "* It will be convenient to decompose the prior as follows (assuming independence of $\\beta$ and $h$): $p(\\beta, h) = p(\\beta | h) p(h)$\n",
    "\n",
    "* Then we can write a prior for $\\beta | h$ and one for $h$\n",
    "\n",
    "* Because we selected a Normal distribution for the likelihood function, a natural conjugate prior will also be Normal.\n",
    "\n",
    "* $\\beta | h$ will have a Normal distribution\n",
    "\n",
    "* But $h$ will have a Gamma distribution\n",
    "\n",
    "* Taken together we will call this the _Normal-Gamma_ prior\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\Large{\\begin{align}\n",
    "\\beta | h & \\sim N(\\underline{\\beta}, h^{-1} \\underline{V})\n",
    "& \\\\\n",
    "& \\\\\n",
    "h & \\sim G(\\underline{s}^{-2}, \\underline{\\nu})\n",
    "\\end{align}}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "This is all denoted by:\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\Large{\\beta, h \\sim NG(\\underline{\\beta}, \\underline{V}, \\underline{s}^{-2}, \\underline{\\nu})}\n",
    "$$\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "* $\\underline{\\beta}$, $\\underline{V}$, $\\underline{s}^{-2}$, $\\underline{\\nu}$ are called _hyperparameters_ \n",
    "\n",
    "* We will need to select values for these to reflect the prior information\n",
    "\n",
    "* The specific role of these parameters becomes more clear when we see how they show up in the posterior\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bd070a",
   "metadata": {},
   "source": [
    "## __Section 2.4: The Posterior__\n",
    "\n",
    "<br>\n",
    "\n",
    "* The posterior summarizes the information that we have (both prior and data-based) about the unknown parameters $\\beta$ and $h$\n",
    "\n",
    "* It is proportional to the likelihood times the prior \n",
    "\n",
    "* For the sake of simplicity we will not work out the gory algebraic details \n",
    "\n",
    "* After the messy mathematical manipulations it can be shown that the posterior is also _Normal-Gamma_\n",
    "\n",
    "<br>\n",
    "\n",
    "Formally, we have \n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\Large{\\beta, h | y \\sim NG(\\bar{\\beta}, \\bar{V}, \\bar{s}^{-2}, \\bar{\\nu})}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "\n",
    "Where\n",
    "\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\Large{\\begin{align}\n",
    "\\bar{V} &= \\frac{1}{\\underline{V}^{-1} + \\sum x_{i}^{2}} \\\\\n",
    "& \\\\\n",
    "& \\\\\n",
    "\\bar{\\beta} &= \\bar{V}(\\underline{V}^{-1}\\underline{\\beta} + \\hat{\\beta} \\sum x_{i}^{2}) \\\\\n",
    "& \\\\\n",
    "& \\\\\n",
    "\\bar{\\nu} &= \\underline{\\nu} + N\n",
    "\\end{align}}\n",
    "$$\n",
    "\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24e04e7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Credit where credit is due: https://github.com/tonyduan/conjugate-bayes/blob/master/conjugate_bayes/models.py\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9466776a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NIGLinearRegression(object):\n",
    "    \"\"\"\n",
    "    The normal inverse-gamma prior for a linear regression model with unknown\n",
    "    variance and unknown relationship. Specifically,\n",
    "        1/σ² ~ Γ(a, b)\n",
    "        β ~ N(0, σ²V)\n",
    "    Parameters\n",
    "    ----------\n",
    "    mu: prior for N(mu, v) on the model β\n",
    "    v:  prior for N(mu, v) on the model β\n",
    "    a:  prior for Γ(a, b) on the inverse sigma2 of the distribution\n",
    "    b:  prior for Γ(a, b) on the inverse sigma2 of the distribution\n",
    "    \"\"\"\n",
    "    def __init__(self, mu, v, a, b):\n",
    "        self.__dict__.update({\"mu\": mu, \"v\": v, \"a\": a, \"b\": b})\n",
    "\n",
    "    def fit(self, x_tr, y_tr):\n",
    "        m, _ = x_tr.shape\n",
    "        mu_ast = np.linalg.inv(np.linalg.inv(self.v) + x_tr.T @ x_tr) @ \\\n",
    "                 (np.linalg.inv(self.v) @ self.mu + x_tr.T @ y_tr)\n",
    "        v_ast = np.linalg.inv(np.linalg.inv(self.v) + x_tr.T @ x_tr)\n",
    "        a_ast = self.a + 0.5 * m\n",
    "        b_ast = self.b + 0.5 * (y_tr - x_tr @ self.mu).T @ \\\n",
    "                np.linalg.inv(np.eye(m) + x_tr @ self.v @ x_tr.T) @ \\\n",
    "                (y_tr - x_tr @ self.mu.T)\n",
    "        self.__dict__.update({\"mu\": mu_ast, \"v\": v_ast, \"a\": a_ast, \"b\": b_ast})\n",
    "\n",
    "    def predict(self, x_te):\n",
    "        scales = np.array([x.T @ self.v @ x for x in x_te]) + 1\n",
    "        scales = (self.b / self.a * scales) ** 0.5\n",
    "        return sp.stats.t(df=2 * self.a, loc=x_te @ self.mu, scale=scales)\n",
    "\n",
    "    def get_conditional_beta(self, sigma2):\n",
    "        return sp.stats.multivariate_normal(mean=self.mu, cov=sigma2 * self.v)\n",
    "\n",
    "    def get_marginal_sigma2(self):\n",
    "        return sp.stats.invgamma(self.a, scale=self.b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea659cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
